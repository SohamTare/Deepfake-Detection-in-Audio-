{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b547bf00",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the 'resampy' library, which is used for audio resampling\n",
    "!pip install resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d35de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import IPython\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda2215",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "audio_files_path = \"Path to your dataset\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess audio files using librosa\n",
    "folders = os.listdir(audio_files_path)\n",
    " print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths to real and fake audio file\n",
    "real_audio = \"Path to real audio from dataset\"\n",
    "fake_audio = \"Path to fake audio from dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d86274",
   "metadata": {},
   "source": [
    "# Visualization of Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ebadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print real Audio file\n",
    "print(\"Real Audio:\")\n",
    "IPython.display.Audio(real_audio) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print fake Audio file\n",
    "print(\"Fake Audio:\")\n",
    "IPython.display.Audio(fake_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27854f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize the waveform of a real audio sample\n",
    "real_ad, real_sr = librosa.load(real_audio)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(real_ad)\n",
    "plt.title(\"Real Audio Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb1fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the spectrogram of a real audio sample\n",
    "real_spec = np.abs(librosa.stft(real_ad))\n",
    "real_spec = librosa.amplitude_to_db(real_spec, ref=np.max)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(real_spec, sr=real_sr, x_axis=\"time\", y_axis=\"log\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Real Audio Spectogram\")\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee5415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the Mel spectrogram of a real audio sample\n",
    "real_mel_spect = librosa.feature.melspectrogram(y=real_ad, sr=real_sr)\n",
    "real_mel_spect = librosa.power_to_db(real_mel_spect, ref=np.max)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(real_mel_spect, y_axis=\"mel\", x_axis=\"time\")\n",
    "plt.title(\"Real Audio Mel Spectogram\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the chromagram of a real audio sample\n",
    "real_chroma = librosa.feature.chroma_cqt(y=real_ad, sr=real_sr, bins_per_octave=36)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(real_chroma, sr=real_sr, x_axis=\"time\", y_axis=\"chroma\", vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.title(\"Real Audio Chromagram\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e400a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MFCC features from the audio and display them as a time-series spectrogram.\n",
    "real_mfccs = librosa.feature.mfcc(y=real_ad, sr=real_sr)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(real_mfccs, sr=real_sr, x_axis=\"time\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Real Audio Mel-Frequency Cepstral Coefficients (MFCCs)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot the fake audio data to visualize the waveform.\n",
    "fake_ad, fake_sr = librosa.load(fake_audio)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(fake_ad)\n",
    "plt.title(\"Fake Audio Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651aed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display the spectrogram of the fake audio using the Short-Time Fourier Transform (STFT).\n",
    "fake_spec = np.abs(librosa.stft(fake_ad))\n",
    "fake_spec = librosa.amplitude_to_db(fake_spec, ref=np.max)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(fake_spec, sr=fake_sr, x_axis=\"time\", y_axis=\"log\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(\"Fake Audio Spectogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display the spectrogram of the fake audio using the Short-Time Fourier Transform (STFT).\n",
    "fake_mel_spect = librosa.feature.melspectrogram(y=fake_ad, sr=fake_sr)\n",
    "fake_mel_spect = librosa.power_to_db(fake_mel_spect, ref=np.max)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(fake_mel_spect, y_axis=\"mel\", x_axis=\"time\")\n",
    "plt.title(\"Fake Audio Mel Spectogram\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad4e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize the chromagram of the fake audio to show pitch class distribution over time.\n",
    "fake_chroma = librosa.feature.chroma_cqt(y=fake_ad, sr=fake_sr, bins_per_octave=36)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(fake_chroma, sr=fake_sr, x_axis=\"time\", y_axis=\"chroma\", vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.title(\"Fake Audio Chromagram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe825fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and display the MFCCs of the fake audio to analyze its timbral features over time.\n",
    "fake_mfccs = librosa.feature.mfcc(y=fake_ad, sr=fake_sr)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(fake_mfccs, sr=fake_sr, x_axis=\"time\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Fake Audio Mel-Frequency Cepstral Coefficients (MFCCs)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b0692d",
   "metadata": {},
   "source": [
    "# Preprocessing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio files from each folder, extract MFCC features, and store them with their corresponding labels.  \n",
    "data = []\n",
    "labels = []\n",
    "for folder in folders:\n",
    "files = os.listdir(os.path.join(audio_files_path, folder))\n",
    "for file in tqdm(files):\n",
    "file_path = os.path.join(audio_files_path, folder, file)\n",
    "audio, sample_rate = librosa.load(file_path, res_type=\"kaiser_fast\")\n",
    "mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "mfccs_features_scaled = np.mean(mfccs_features.T, axis=0)\n",
    "data.append(mfccs_features_scaled)\n",
    "labels.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b69d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to organize the extracted features and their corresponding class labels.  \n",
    "feature_df = pd.DataFrame({\"features\": data, \"class\": labels})\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4760ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the count of samples for each class label in the dataset.  \n",
    "feature_df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c14f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical class labels into numerical format using LabelEncoder.  \n",
    "def label_encoder(column):\n",
    "    le = LabelEncoder().fit(column)\n",
    "    print(column.name, le.classes_)\n",
    "    return le.transform(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc28a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to convert class labels in the DataFrame to numeric values.  \n",
    "feature_df[\"class\"] = label_encoder(feature_df[\"class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d34db07",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the features and class labels from the DataFrame into NumPy arrays for model training.  \n",
    "X = np.array(feature_df[\"features\"].tolist())\n",
    "y = np.array(feature_df[\"class\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply random oversampling to balance the class distribution in the dataset.  \n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the resampled class labels to one-hot encoded format for model training.  \n",
    "y_resampled = to_categorical(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a523a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the resampled dataset into training and testing sets (80% train, 20% test).  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of unique class labels in the dataset.  \n",
    "num_labels = len(feature_df[\"class\"].unique())\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the shape of a single feature vector to define the input shape for the model.  \n",
    "input_shape = feature_df[\"features\"][0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e194b",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network model with multiple layers, including dense, activation, and dropout layers for regularization.\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=input_shape))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation(activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c35d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using categorical crossentropy loss, Adam optimizer, and accuracy as the evaluation metric.\\\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae009701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the summary of the model architecture, including layer details and parameter counts.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7911a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the training data with validation, specifying batch size and number of epochs.\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=2, epochs=100) # callbacks=[early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the test set and print the test loss and accuracy.\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead07f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy over epochs to visualize model performance.\n",
    "plt.figure()\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"validation\")\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a80b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss over epochs to visualize model convergence.\n",
    "plt.title(\"Model Loss\")\n",
    "plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"validation\")\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33538e",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac08956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to detect whether an audio file is fake or real using the trained model and MFCC features.\n",
    "def detect_fake(filename):\n",
    "sound_signal, sample_rate = librosa.load(filename, res_type=\"kaiser_fast\")\n",
    "mfcc_features = librosa.feature.mfcc(y=sound_signal, sr=sample_rate, n_mfcc=40)\n",
    "mfccs_features_scaled = np.mean(mfcc_features.T, axis=0)\n",
    "mfccs_features_scaled = mfccs_features_scaled.reshape(1, -1)\n",
    "result_array = model.predict(mfccs_features_scaled)\n",
    "print(result_array)\n",
    "result_classes = [\"FAKE\", \"REAL\"]\n",
    "result = np.argmax(result_array[0])\n",
    "print(\"Result:\", result_classes[result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for the real and fake audio samples to be tested.\n",
    "test_real = \"Path to Original audio from Dataset\"\n",
    "test_fake = \"Path to Original audio from Dataset\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839cfa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the detect_fake function to predict whether the real audio sample is fake or real.\n",
    "detect_fake(test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03462f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the detect_fake function to predict whether the fake audio sample is fake or real.\n",
    "detect_fake(test_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e421e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
